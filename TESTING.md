# Testing Framework for Tensor-Truth

## Overview

This document outlines the testing strategy for the tensor-truth RAG pipeline project.

## Testing Stack

- **pytest**: Primary testing framework
- **pytest-cov**: Code coverage reporting
- **pytest-mock**: Mocking support
- **pytest-asyncio**: Async test support (for future async features)
- **hypothesis**: Property-based testing for edge cases
- **responses**: HTTP request mocking
- **faker**: Test data generation

## Installation

```bash
pip install pytest pytest-cov pytest-mock pytest-asyncio hypothesis responses faker
```

## Project Structure

```
tensor-truth/
â”œâ”€â”€ src/tensortruth/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_db.py
â”‚   â”œâ”€â”€ fetch_paper.py
â”‚   â”œâ”€â”€ rag_engine.py
â”‚   â”œâ”€â”€ scrape_docs.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures and configuration
â”‚   â”œâ”€â”€ unit/                     # Unit tests (isolated functions)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_utils.py
â”‚   â”‚   â”œâ”€â”€ test_fetch_paper.py
â”‚   â”‚   â”œâ”€â”€ test_scrape_docs.py
â”‚   â”‚   â””â”€â”€ test_rag_engine.py
â”‚   â”œâ”€â”€ integration/              # Integration tests (components together)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_ingestion_pipeline.py
â”‚   â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â”‚   â””â”€â”€ test_db_building.py
â”‚   â”œâ”€â”€ e2e/                      # End-to-end tests
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ test_app.py
â”‚   â””â”€â”€ fixtures/                 # Test data and fixtures
â”‚       â”œâ”€â”€ sample_papers/
â”‚       â”œâ”€â”€ sample_configs/
â”‚       â””â”€â”€ mock_responses/
â”œâ”€â”€ pytest.ini                    # Pytest configuration
â””â”€â”€ .coveragerc                   # Coverage configuration
```

## Test Categories

### 1. Unit Tests (Fast, No External Dependencies)

#### `test_utils.py`
- âœ… `parse_thinking_response()` - Various input formats
- âœ… `get_max_memory_gb()` - CUDA/MPS/CPU detection
- âœ… `convert_chat_to_markdown()` - Session formatting
- âœ… `stop_model()` - API interaction (mocked)
- âœ… `get_running_models()` - Ollama API parsing (mocked)

#### `test_fetch_paper.py`
- âœ… `clean_filename()` - Sanitization edge cases
- âœ… `paper_already_processed()` - File detection
- âœ… `book_already_processed()` - Book detection
- âœ… `extract_toc()` - PDF TOC parsing (mocked PDF)
- âœ… `post_process_math()` - Math symbol conversion
- âœ… `url_to_filename()` - URL sanitization

#### `test_scrape_docs.py`
- âœ… `clean_doxygen_html()` - HTML cleanup
- âœ… `url_to_filename()` - Filename generation
- âœ… `load_config()` - JSON parsing
- âœ… `detect_category_type()` - Papers vs books

#### `test_rag_engine.py`
- âœ… `MultiIndexRetriever._retrieve()` - Query routing
- âš ï¸  `get_embed_model()` - Model initialization (mock heavy deps)
- âš ï¸  `get_llm()` - LLM config validation (mock Ollama)
- âš ï¸  `get_reranker()` - Reranker initialization (mock model)

### 2. Integration Tests (Moderate, Local Resources)

#### `test_ingestion_pipeline.py`
- âš™ï¸  Fetch paper â†’ Convert â†’ Index (small test paper)
- âš™ï¸  Book splitting (TOC, manual, none)
- âš™ï¸  Config-based category rebuilding

#### `test_rag_pipeline.py`
- âš™ï¸  Load index â†’ Query â†’ Retrieve sources
- âš™ï¸  Multi-index retrieval merging
- âš™ï¸  Reranking effectiveness

#### `test_db_building.py`
- âš™ï¸  Build module with sample docs
- âš™ï¸  Hierarchical node parsing
- âš™ï¸  ChromaDB persistence

### 3. End-to-End Tests (Slow, Full System)

#### `test_app.py`
- ğŸ”„ Session creation flow
- ğŸ”„ Chat interaction (mocked LLM)
- ğŸ”„ Preset save/load
- ğŸ”„ Command processing (/load, /status, etc.)
- ğŸ”„ Memory management

## Test Fixtures (conftest.py)

```python
# Sample fixtures to be created:

@pytest.fixture
def sample_paper_metadata():
    """Mock arXiv paper metadata"""

@pytest.fixture
def sample_pdf_path(tmp_path):
    """Generate a minimal test PDF"""

@pytest.fixture
def mock_ollama_api(responses):
    """Mock Ollama API responses"""

@pytest.fixture
def mock_chroma_db(tmp_path):
    """Temporary ChromaDB instance"""

@pytest.fixture
def sample_markdown_content():
    """Sample markdown for testing parsers"""

@pytest.fixture
def mock_embedding_model():
    """Mock HuggingFace embedding model"""
```

## Testing Priorities by Component

### High Priority (Critical Path)
1. âœ… **utils.py** - Core utilities, easy to test
2. âœ… **fetch_paper.py** - Data ingestion, many edge cases
3. âš™ï¸ **build_db.py** - Index building logic
4. ğŸ”„ **app.py** - User-facing commands and session management

### Medium Priority
5. âš ï¸ **rag_engine.py** - Requires mocking heavy ML models
6. âš™ï¸ Integration tests - End-to-end validation

### Lower Priority (Complex Mocking)
7. ğŸ”„ **scrape_docs.py** - Network-dependent, best tested with fixtures
8. ğŸ”„ Full E2E with Streamlit rendering

## Testing Best Practices

### Mocking Strategy
- **External APIs**: Mock all Ollama, arXiv, web scraping calls
- **ML Models**: Mock HuggingFace models, use small test embeddings
- **File System**: Use `tmp_path` fixtures for temp files
- **ChromaDB**: Use in-memory or temp directory instances

### Test Isolation
- Each test should be independent
- Use `setUp`/`tearDown` or fixtures for cleanup
- No shared state between tests

### Property-Based Testing
Use `hypothesis` for functions with complex input spaces:
```python
from hypothesis import given, strategies as st

@given(st.text(min_size=1, max_size=100))
def test_clean_filename_never_crashes(title):
    result = clean_filename(title)
    assert isinstance(result, str)
    assert len(result) <= 50
```

## Coverage Targets

- **Overall**: 80%+
- **Utils**: 95%+ (pure functions, easy to test)
- **RAG Engine**: 70%+ (mock-heavy)
- **Integration**: 60%+ (system-dependent)

## Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=tensortruth --cov-report=html

# Run specific category
pytest tests/unit/
pytest tests/integration/

# Run specific file
pytest tests/unit/test_utils.py

# Run with verbose output
pytest -v

# Run with markers
pytest -m "not slow"  # Skip slow tests

# Run failed tests only
pytest --lf

# Run in parallel (requires pytest-xdist)
pytest -n auto
```

## CI/CD Integration

### GitHub Actions Example
```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        pip install -e .
        pip install pytest pytest-cov
    - name: Run tests
      run: pytest --cov=tensortruth --cov-report=xml
    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

## Test Data Management

### Fixtures Directory Structure
```
tests/fixtures/
â”œâ”€â”€ sample_papers/
â”‚   â”œâ”€â”€ sample.pdf          # Minimal test PDF
â”‚   â””â”€â”€ sample.md           # Expected markdown output
â”œâ”€â”€ sample_configs/
â”‚   â”œâ”€â”€ papers.json         # Test paper config
â”‚   â””â”€â”€ books.json          # Test book config
â”œâ”€â”€ mock_responses/
â”‚   â”œâ”€â”€ ollama_models.json  # Mock Ollama API
â”‚   â””â”€â”€ arxiv_metadata.json # Mock arXiv response
â””â”€â”€ sample_docs/
    â”œâ”€â”€ pytorch_sample.html
    â””â”€â”€ numpy_sample.html
```

## Next Steps

1. âœ… Create base testing infrastructure (pytest.ini, conftest.py)
2. âœ… Implement high-priority unit tests (utils.py)
3. âš™ï¸ Add integration tests for core workflows
4. ğŸ”„ Set up CI/CD pipeline
5. ğŸ“Š Establish coverage baseline and targets

## Legend
- âœ… Easy to implement, high value
- âš™ï¸ Moderate complexity, requires setup
- âš ï¸ Requires extensive mocking
- ğŸ”„ Complex, end-to-end scenarios
